{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuh3gf7s6Cj0erA4MfVJg2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/catalinapesquet/PINNS-Code-and-Notes/blob/main/06_PyTorch_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. PyTorch Transfer Learning\n",
        "\n",
        "## What is transfer learning?\n",
        "Transfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n",
        "\n",
        "For example, we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power a FoodVision Mini model.\n",
        "\n",
        "## Why use transfer learning?\n",
        "There are two main benefits to using transfer learning:\n",
        "\n",
        "1. Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\n",
        "2. Can leverage a working model which has already learned patterns on similar data to our own. This often results in achieving great results with less custom data.\n"
      ],
      "metadata": {
        "id": "KGAMh9-XiOav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where to find pretrainded models\n",
        "\n",
        "* PyTorch domain libraries: each of domain libraries come with pretrained models\n",
        "* HuggingFace Hub: pretrained models on many different domains from around the world, plenty of datasets too\n",
        "* timm(PyTorch Image Models library): all latest and greatest computer vision models in PyTorch code\n",
        "* Paperswithcode: collection of latest state-of-the-art ML papers with code implementations attached"
      ],
      "metadata": {
        "id": "ld-V9HRgjCMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Getting setup\n",
        "\n",
        "Let's get started by importing/downloading the required modules for this section.\n",
        "\n",
        "To save us writing extra code, we're going to be leveraging some of the Python scripts (such as data_setup.py and engine.py) we created in the previous section, 05. PyTorch Going Modular."
      ],
      "metadata": {
        "id": "VmGYCQk8j_mp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FzNH1YKjiKlV",
        "outputId": "0de12f57-f660-419b-8281-57597b645bbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "torch version: 2.5.1+cu121\n",
            "torchvision version: 0.20.1+cu121\n"
          ]
        }
      ],
      "source": [
        "try: # let's us test a block of code for errors\n",
        "    import torch\n",
        "    import torchvision\n",
        "    # assert: test if a condition is true\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except: # let's us handle the error\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regular Imports"
      ],
      "metadata": {
        "id": "JT0r2mE5czp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesnt work\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo...Installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "aLk5ig1Gc1DC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d81942-29dd-4bb6-ac2c-883154494a9b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo...Installing it.\n",
            "[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4356, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 4356 (delta 154), reused 120 (delta 120), pack-reused 4171 (from 3)\u001b[K\n",
            "Receiving objects: 100% (4356/4356), 654.37 MiB | 22.69 MiB/s, done.\n",
            "Resolving deltas: 100% (2584/2584), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's setup device agnostic code."
      ],
      "metadata": {
        "id": "KvumrCLadaBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EXtocJEZdfvs",
        "outputId": "b9110240-3078-44ac-e638-40c502827b4c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get data\n",
        "\n",
        "We first need a Dataset.\n",
        "To compare with a model we'll download the same dataset we've been using for FoodVision Mini."
      ],
      "metadata": {
        "id": "L1gVOZAAfhRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it.\n",
        "if image_path.is_dir():\n",
        "  print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "  print(f\"Did not find {image_path} directory, creating one...\")\n",
        "  image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # Download pizza, steak, sushi data\n",
        "  with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "  # Unzip pizza, steak, sushi data\n",
        "  with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "  # Remove .zip file\n",
        "  os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZsbTazMfwsn",
        "outputId": "2dd1f05b-7e08-4d32-c689-3ae6bc521d1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got the same dataset we've been using before.\n",
        "Let's create paths to our training and test directories"
      ],
      "metadata": {
        "id": "GiAqYVV8g_m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Dirs\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "zQmafR62hPgU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Datasets and DataLoaders\n",
        "\n",
        "Since we've downloaded going_modular we can use the data_setup.py script we created before to prepare and setup our DataLoaders.\n",
        "\n",
        "But since we'll be using a pretrained model from torchvision.models, ther's a specific transform we need to prepare our images first.\n",
        "\n"
      ],
      "metadata": {
        "id": "xVn_rZ41hVwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Creating a transform for torchvision.models (manual creation)\n",
        "\n",
        "When using a pretrained model, it's important that the custom data going into the model is prepared in the same way as the original training data that went into the model.\n",
        "\n",
        "\n",
        "\n",
        "The documentation states:\n",
        "\n",
        "*All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224.*\n",
        "\n",
        "*The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225].*"
      ],
      "metadata": {
        "id": "dcRQ-EjKhtn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can achieve the above transformations with a combination of:\n",
        "1. Mini-batches of size [batch_size, 3, height, width]: torchvision.transforms.Resize() + torch.utils.data.DataLoader()\n",
        "2. Values between 0 and 1: torchvision.transforms.ToTensor()\n",
        "3. A mean of [0.485, 0.456, 0.406]: torchvision.transforms.Normalize(mean=...)\n",
        "4. A standard deviation of [0.229, 0.224, 0.225]: torchvision.transforms.Normalize(std=...)"
      ],
      "metadata": {
        "id": "fawdoYQGihSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transforms pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "EPLZt6kvjMy0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create training and testing DataLoaders usin create_dataloaders function from the data_setup.py script.\n",
        "\n",
        "create_dataloader takes in a training and testing path and turns them into PyTorch Datasets and then into PyTorch DataLoaders."
      ],
      "metadata": {
        "id": "L-NBmSIQk23c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
        "                                                                               batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to_HCKBGlGKc",
        "outputId": "edd43c58-762b-44a2-fd76-1fbae374adbb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7e9ee1ab6200>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7e9e1ff32aa0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Creating a transform for torchvision.models (auto creation)"
      ],
      "metadata": {
        "id": "y6boqsNWlJVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using a pretrained model we need to prepare our custom data in the same way as the original training data that went into the model.\n",
        "\n",
        "Above we saw how to manually create a transform for a pretrained model.\n",
        "\n",
        "But as of torchvision v0.13+, an automatic transform creation feature has been added."
      ],
      "metadata": {
        "id": "opfG8n7zplMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT"
      ],
      "metadata": {
        "id": "M_YTFUB8p4HC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Where:\n",
        "* EfficientNet_B0_Weights: model architecture weights we'd like to use.\n",
        "* DEFAULT means the best available weights (the best performance in ImageNet).\n",
        "\n",
        "Let's try it out."
      ],
      "metadata": {
        "id": "JRs645Q5p7IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of pretrained model weights\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3QDTS-SqMPK",
        "outputId": "6cdfa1f1-6e8f-4c85-a290-304d118286e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet_B0_Weights.IMAGENET1K_V1"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now to access the transforms associated with our weights, we can use the transforms() method."
      ],
      "metadata": {
        "id": "DdOGpRuCqZ-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the transforms used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYSpd7leqhkx",
        "outputId": "f38942b5-5a96-4b05-e130-51d00353dd5b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use auto_transforms to create DataLoaders with create_dataloaders() just as before."
      ],
      "metadata": {
        "id": "NUucZKo4rPaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=auto_transforms,\n",
        "                                                                               batch_size=32)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRxSIf0CrVCr",
        "outputId": "4ea19074-0742-4a1c-98bb-3af4bb6063e6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7e9e20621900>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7e9e20623070>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Getting a pretrained model\n",
        "\n",
        "Since we're working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in torchvision.models."
      ],
      "metadata": {
        "id": "ACYoPM8Tr0Eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Which pretrained model should we use ?\n",
        "\n",
        "Depends on the problem/device we're working on.\n",
        "\n",
        "You might think better performance is always better, right?\n",
        "\n",
        "That's true but some better performing models are too big for some devices.\n",
        "\n",
        "For example, say we'd like to run our model on a mobile-device, we'll have to take into account the limited compute resources on the device, thus we'd be looking for a smaller model."
      ],
      "metadata": {
        "id": "bH8qKRbnsDLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Setting up a pretrained model\n",
        "\n",
        "The pretrained model we're going to be using is torchvision.models.efficientnet_b0().\n",
        "\n",
        "This means the model has already been trained on millions of images and has a good base representation of image data.\n",
        "\n",
        "We send it to the target device:"
      ],
      "metadata": {
        "id": "fnQt1N7hsicF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "\n",
        "#model # uncomment to output (it's very long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ybYVFlms3sg",
        "outputId": "9cd37477-489c-4e96-ffee-56e5aeb0cc23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 67.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we print the model, we get something similar to the following:Lots and lots and lots of layers.\n",
        "\n",
        "This is one of the benefits of transfer learning, taking an existing model, that's been crafted by some of the best engineers in the world and applying to our own problem.\n",
        "\n",
        "The efficientnet_b0 comes in three main parts:\n",
        "1. features: A collection of convolutional layers and other various activation layers to learn a base representation of vision data\n",
        "2. avgpool: Takes the average of the output of the features layer(s) and turns it into a feature vector.\n",
        "3. classifier: Turns the feature vector into a vector with the same dimensionality as the number of required output classes"
      ],
      "metadata": {
        "id": "8N1L4G4AuTsc"
      }
    }
  ]
}